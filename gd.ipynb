{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28f14f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a5f7ed",
   "metadata": {},
   "source": [
    "Queremos minimizar la función de pérdida:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7422a5",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sum_{i=1}^{N} \\left( f^*(\\mathbf{i}_i) - d_i \\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9d52f2",
   "metadata": {},
   "source": [
    "Es decir, queremos minimizar la diferencia entre los diagnósticos que predice el modelo y los reales. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044668eb",
   "metadata": {},
   "source": [
    "Buscamos la mejor solución dentro de las funciones $f : \\mathbb{R}^K \\to (0, 1)$\n",
    " que tengan la forma:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43675cbf",
   "metadata": {},
   "source": [
    "$$\n",
    "f_{\\mathbf{w}, b}(\\mathbf{i}) = \\frac{\\tanh(\\mathbf{w} \\cdot \\mathbf{i} + b) + 1}{2}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539bda14",
   "metadata": {},
   "source": [
    "donde $w$ es un vector de pesos (weights) de $\\mathbb{R}^K$ , $b$ (bias) un escalar, y $tanh$ la tangente hiperbólica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee8a456",
   "metadata": {},
   "source": [
    "## Preprocesamiento de imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee9a5e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_convert(dir, new_size):\n",
    "    \n",
    "    imagenes = []\n",
    "\n",
    "    for filename in os.listdir(dir):\n",
    "\n",
    "        file_path = os.path.join(dir, filename)\n",
    "        # print(f\"Procesando imagen {file_path}\")\n",
    "        \n",
    "        if os.path.isfile(file_path):\n",
    "\n",
    "            # Sin este if no funciona en mi windows, probar comentarlo en Mac a ver que pasa\n",
    "            if not filename.lower().endswith('.png'):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                with Image.open(file_path) as img:\n",
    "                    img = img.resize(new_size) \n",
    "                    img = img.convert('L') # Convertir a escala de grises\n",
    "                    img_array = np.array(img)/255.0 # Convertir a array y normalizar\n",
    "                    img_vector = img_array.reshape((new_size[0]**2)) \n",
    "                    imagenes.append(img_vector)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error procesando la imagen {file_path}: {e}\")\n",
    "\n",
    "    return np.array(imagenes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21cc78a",
   "metadata": {},
   "source": [
    "Convertimos las imágenes a arreglos de pixeles y separamos en train y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a06e2a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "original = (256, 256)\n",
    "mediano =  (128, 128)\n",
    "chico = (64, 64)\n",
    "muy_chico = (32, 32)\n",
    "\n",
    "size = chico  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a5737d",
   "metadata": {},
   "source": [
    "En el dataset provisto tenemos 1617 dibujos de pacientes sanos y 1629 dibujos de pacientes con parkinson.\n",
    "\n",
    "Cómo armamos el conjunto de entrenamiento?\n",
    "* Tiene que tener cantidades parecidas de pacientes con y sin parkinson.\n",
    "* Aproximadamente el 70% del total de observaciones deberían ir al conjunto de entrenamiento\n",
    "\n",
    "Podemos hacerlo asi: \\\n",
    "train \\\n",
    "sanos: 1132 \\\n",
    "enfermos: 1140 \n",
    "\n",
    "test \\\n",
    "sanos: 485 \\\n",
    "enfermos: 489"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53e32045",
   "metadata": {},
   "outputs": [],
   "source": [
    "cant_healthy_train = 1132\n",
    "cant_park_train = 1140\n",
    "\n",
    "cant_healthy_test = 485\n",
    "cant_park_test = 489"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26aa8b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Convierto todas las imágenes a arreglos de pixeles según el tamaño elegido\n",
    "\n",
    "# Healthy\n",
    "src_dir = 'DatasetTP/Healthy'\n",
    "i_healthy = image_convert(src_dir, size) # Imágenes de pacientes sanos\n",
    "d_healthy = np.ones((i_healthy.shape[0], 1)) * 0 # Vector de diagnósticos para la gente sana (0)\n",
    "\n",
    "# Parkinson\n",
    "src_dir = 'DatasetTP/Parkinson'  \n",
    "i_park = image_convert(src_dir, size) # Imágenes de pacientes con Parkinson\n",
    "d_park = np.ones((i_park.shape[0], 1)) # Vector de diagnósticos para la gente con Parkinson (1)\n",
    "\n",
    "# Separo en train y test\n",
    "i_healthy_train = i_healthy[:cant_healthy_train]\n",
    "d_healthy_train = d_healthy[:cant_healthy_train]\n",
    "\n",
    "i_healthy_test = i_healthy[cant_healthy_train:cant_healthy_train + cant_healthy_test]\n",
    "d_healthy_test = d_healthy[cant_healthy_train:cant_healthy_train + cant_healthy_test]\n",
    "\n",
    "i_park_train = i_park[:cant_park_train]\n",
    "d_park_train = d_park[:cant_park_train]\n",
    "\n",
    "i_park_test = i_park[cant_park_train:cant_park_train + cant_park_test]\n",
    "d_park_test = d_park[cant_park_train:cant_park_train + cant_park_test]\n",
    "\n",
    "# Combino sanos y enfermos en train y test\n",
    "i_train = np.vstack((i_healthy_train, i_park_train)) # Imágenes de entrenamiento\n",
    "d_train = np.vstack((d_healthy_train, d_park_train)) # Diagnósticos de entrenamiento\n",
    "\n",
    "i_test = np.vstack((i_healthy_test, i_park_test)) # Imágenes de test\n",
    "d_test = np.vstack((d_healthy_test, d_park_test)) # Diagnósticos de test\n",
    "\n",
    "# Mezclo los datos de entrenamiento y test\n",
    "np.random.seed(42)  \n",
    "np.random.shuffle(i_train)\n",
    "np.random.shuffle(d_train)\n",
    "np.random.shuffle(i_test)\n",
    "np.random.shuffle(d_test)\n",
    "\n",
    "# Chequeamos que la intersección entre train y test sea nula (Importante)\n",
    "print(len(set(map(tuple, i_train)).intersection(map(tuple, i_test))) == 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c7f2d6",
   "metadata": {},
   "source": [
    "## Derivadas parciales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1c3236",
   "metadata": {},
   "source": [
    "$$\n",
    "\\arg\\min_{\\mathbf{w}, b} \\mathcal{L}(\\mathbf{w}, b) = \n",
    "\\arg\\min_{\\mathbf{w}, b} \\sum_{i=1}^{N} \\left( f_{\\mathbf{w}, b}(\\mathbf{i}_i) - d_i \\right)^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af100edf",
   "metadata": {},
   "source": [
    "Derivamos $\\mathcal{L}$ con respecto a $b$ (bias) y $w$ (weight):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e728be50",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\mathcal{L}(\\mathbf{w}, b) }{\\partial w} = (1-\\tanh(b+w^\\top \\cdot i)^{2})\\cdot ((1+\\tanh(b+w^\\top \\cdot i))/2-d)\\cdot i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e8ed65",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\mathcal{L}(\\mathbf{w}, b) }{\\partial b} = (1-\\tanh(b+w^\\top \\cdot i)^{2})\\cdot ((1+\\tanh(b+w^\\top \\cdot i))/2-d)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e02539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fwb(i, w, b):\n",
    "    z = i@w + b \n",
    "    f = np.tanh(z) / 2 \n",
    "    return f # devuelve un vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c895da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función a minimizar\n",
    "def argminL(w, b, i, d):\n",
    "    return np.sum((fwb(i, w, b) - d) ** 2) #suma todos los elementos del vector --> devuelve numero "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd7e0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derivada parcial con respecto a w\n",
    "def df_w(w, b, i, d):\n",
    "    z = b + w @ i\n",
    "    return (1 - np.tanh(z) ** 2) * ((1 + np.tanh(z)) / 2 - d) @ i # Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6abd95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derivada parcial con respecto a b\n",
    "def df_b(w, b, i, d):\n",
    "    z = b + w @ i\n",
    "    return (1 - np.tanh(z) ** 2) * ((1 + np.tanh(z)) / 2 - d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87d36cf",
   "metadata": {},
   "source": [
    "## Gradiente Descendiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b39ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Máxima cantidad de iteraciones (previene loops infinitos)\n",
    "MAX_ITER = 1000\n",
    "\n",
    "# Criterio de convergencia (identifica un \"plateau\")\n",
    "TOLERANCIA = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761cd096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradiente_descendente(b, w, imagenes_train, diagnosticos_train, imagenes_test, diagnosticos_test, alpha=0.1):\n",
    "    \n",
    "    iter  = 0\n",
    "    valores_train = []\n",
    "    valores_test = []\n",
    "\n",
    "    while iter <= MAX_ITER:\n",
    "        print(\"Iteración: \", iter, \"- Mínimo alcanzado hasta el momento: \", argminL(w, b, imagenes_train, diagnosticos_train))\n",
    "\n",
    "        valores_train.append(argminL(w, b, imagenes_train, diagnosticos_train))\n",
    "\n",
    "        if imagenes_test is not None and diagnosticos_test is not None: \n",
    "          valores_test.append(argminL(w, b, imagenes_test, diagnosticos_test))\n",
    "\n",
    "        gradiente_w = df_w(w, b, imagenes_train, diagnosticos_train) \n",
    "        gradiente_b = df_b(w, b, imagenes_train, diagnosticos_train)\n",
    "\n",
    "        w_sig = w - alpha * gradiente_w\n",
    "        b_sig = b - alpha * gradiente_b\n",
    "\n",
    "        criterio = np.abs(argminL(w_sig, b_sig, imagenes_train, diagnosticos_train) - argminL(w, b, imagenes_train, diagnosticos_train))\n",
    "\n",
    "        if criterio < TOLERANCIA: # Converge\n",
    "          break\n",
    "\n",
    "        w = w_sig\n",
    "        b = b_sig\n",
    "        it += 1\n",
    "\n",
    "    return w, b, valores_train, valores_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffe9611",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "td6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
